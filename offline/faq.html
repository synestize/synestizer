<div class="left">
    <img style="margin-bottom: 20px;" src="https://raw.githubusercontent.com/synestize/synestizer/master/docs/synestizer_logo_50x50.png" />
</div>
<div class="right">
    <blockquote>Just open the window... and focus on listening to what emerges outside.<cite>John Cage</cite></blockquote>
    <blockquote>It`s time to open the digital window and listen colourwise.<cite>Koenig</cite></blockquote>
</div>
<div class="right">
    <p>
        This document is made to explain the idea and function of the synestizer project.
        A webapp to listen to the dataflow of the camera-imagery.
    </p>
    <h2>What is a Synestizer?</h2>
    <p>
        The word Synestizer is a blend of the word synthesizer (electric signals converted into sounds) and synaesthesia (sensation together). The synestizer is an attempt to sonify the live imagery of a (web-)camera by making colour-values audible in different ways. The idea behind the synestizer is to let visual and audio-input interact to tickle the synapse between your ears and eyes. In many ways visualizers (including screensavers) do this by transforming sound in to animated or generated graphics. The synestizer tries to do the opposite as it translates Red, Green and Blue values to sound. For this effect or let’s say „sense“ there are new methods and techniques explored, developed and spread online. That's why it is time to open the digital window and listen to it.
    </p>
    <h2>Why working hard to invent a Synestizer?</h2>
    <p>
        Since Cameras can measure Red Green Blue (RGB) values and its luminosity in given situations they correspond to the RGB sensitivity of the cones of the eyes. Since audible and visible frequencies differ in wavelength only, there might be a relation of some sort. The fascinating part of the synestizer is to come up with sonifying techniques to suggest an instant soundtrack matching exactly what you see in front of you right now. Imagining an induced synesthesia by technical means already on an average desk or pocket computer device.
    </p>
    <img class="gfx" src="https://raw.githubusercontent.com/synestize/synestizer/master/docs/Color_Sensitivity.jpg" />
    <p>
        Can this webapp contribute to the moment? Or at least extend the “obvious” with an interesting „odor” or an ambient-lighting but with audible means. In the future we might get used to an add-on sonification to our daily routines. Like a personal contextual-sound-DJ or as an automated soundtrack-maker for movies. Maybe just to pimp a daily situation or create an acoustic ambiance for a room.
    </p>
    <p>A challenge for the synestizer-project is to generate ad-hoc compositions that are fitting to the situation you are or better said your camera is in, and on top surprise you again and again.
    </p>
    <h2>Features</h2>
    <p>
        The screen is divided into 4 sections. The left column is a visual readout of the camera measuring the Red, Green and Blue values in real-time. The background-colour is the representation and intensity of the average colour I=1/3*(R+G+B)  and the middle frame obviously displays what the camera captures. The right frame is the Audio section which enables the user to change the volume-mixture and the sound characteristics.
    </p>
    <h3>Triad</h3>
    <p>
        Direct simple sonification.
    </p>
    <p>
        The RGB values are directly translated into tones. A 50% RGB value has an output of 3 sinewave tones with 440Hz by default.  To differentiate the similar sine-wave sound, we decided to make a square-wave and saw-wave available. On top the lowest pitch can be adopted by the knob located at each colour. Also an LFO oscillates correlates to RGB value.
    </p>
    <h3>Minimal</h3>
    <p>
        The RGB values influence the intervals of the notes played by synthesized instruments available through the implementation of <a href ="https://github.com/hoch/waax">waax</a>.
        According to the score at 50% colour value, the 3 instruments
        (piano = red / pan flute = green / vibraphone = blue)
        play each 3 notes per bar with small intervals in between. Going up and down the scale in a variety of intervals, the color amounts are sonified. (see picture 1). The tempo of the intervals are set with the corresponding knob.
    </p>
    <img class="gfx" src="https://raw.githubusercontent.com/synestize/synestizer/master/docs/Synestizer_Partitur.png" />
    <img class="gfx" src="https://raw.githubusercontent.com/synestize/synestizer/master/docs/Synestizer_sketch_score2.png" />
    <h3>Sampler</h3>
    <p>
        This frame is based on the manipulation of short samples that can be recorded immediately and played back with RGB values influencing speed and pitch.
    </p>
    <h2>Who develops the synestizer?</h2>
    <p>
        Soundartist <a href="http://www.kasparkoenig.com" target="_blank">Kaspar König</a> loves experimenting with analog and digital techniques to sonify the context imminently surrounding us.
        His interest in interpreting colours, patterns, materials and most recently also wind as an actor to map sound lead to the development of the initial version of the synestizer.
        Now the group is growing and there are new people interested in the project, and we have opened up the code to invite even more to join us.
        Users and artists are welcome to experiment with, and developers, to modify, this thing.
    </p>
    <p>
        Now <a href="http://www.stahlnow.com">Christoph Stähli</a> (programmer of Noisetracks and more) and <a href="http://notes.livingthing.org/">Dan MacKinlay</a>, the mathemagician in the team, are committedly contributing a great deal to the code of the project.
        Support has also been provided by the <a href="http://fasos-research.nl/sonicsciencefestival/event/163/?instance_id=98">Sonic Sonic Skills research project at the Faculty of Arts and Social Sciences of Maastricht University</a>, coordinated by Prof. Karin Bijsterveld.
    </p>
    <p>
        Want to join us? See <a href="https://github.com/synestize/synestizer">github</a> for the source code.
    </p>
    <p>
        A big thank you is in order for much previous work that has also been done by: Univ.- Prof. Peter Kiefer, Prof. Dr. Stefan Reuss, Dr. Roman Mauer, Dr. Michael Liegl, Mark Lingk.
        And the <a href="http://www.medienkonvergenz.uni-mainz.de">Forschungsschwerpunkt Medienkonvergenz</a> from the Johannes Gutenberg University in Mainz (D)
        The Dutch “Stichting de 4 Koningen” supported the development of this Web-app.
    </p>
    <p>Ideas or interest in participation? Just write us an email at <a href="mailto:artresearchmusic@gmail.com">artresearchmusic@gmail.com</a></p>
    <h3>Privacy</h3>
    <p>
        Although you have to accept the general use of the camera and microphone in the browser, we do not collect or even receive any data from it.
        You can verify this by going off- and online again and see that, once the app is downloaded, it doesn't use the network connection.
    </p>
    <p>
        The sound and imagery is not captured.
        The (voluntary) option of saving preferences, patches, settings or mappings online at <a href="http://www.synestizer.com">synestizer.com</a> may be added in a future version.
    </p>
    <a class="close-reveal-modal">&#215;</a>
    <a class="close-reveal-modal custom">&#215;</a>
</div>
